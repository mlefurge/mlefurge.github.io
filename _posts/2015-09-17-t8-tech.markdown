---
layout: post
title:  "Technical Entry 8 - Big O Notation"
date:   2015-09-17
categories: blog entry
---
As an aspiring computer programmer, I’m not sure if I should be ashamed or embarrassed, but before today I had never heard of Big O Notation. It also didn’t help that when I did a quick Google search, I was terrified by the results.  Every result seemed to have some complicated graph and a bunch of equations with letters and squiggly lines. There were even a few one-line definitions that threw me for a loop.  Am I that dense that I can’t even begin to understand a fundamental concept in computer programming and mathematics?

After doing a little more digging and a lot more thinking, I believe that I have pieced together enough explanations to understand the basic concept being Big O notation. In a non-intimidating, easy to read and brief sentence, I would describe Big O notation as a way to measure how efficiently an algorithm runs given different inputs.

Okay, so what about all those equations and graphs? Well, the equations and letters represent the different efficiencies of algorithms. I think the best way to understand it is by looking at a few examples. I’ve provided a few of the more common orders of growth below.

<strong>O(1) – Constant time</strong><br></br>
If you see something like “this function runs in O(1) time”, all it’s really saying is that the efficiency of the algorithm remains constant. No matter how big your input is, the algorithm will run in the same amount of time.

<strong>O(N) – Linear time</strong><br></br>
If something runs in O(N) time, it means that the algorithm performance is directly proportional (i.e., linearly) to the size of the input .

<strong>O(N<sup>2</sup>) – Quadratic time</strong><br></br>
Something that runs in quadratic time means the performance is directly proportional to the square of the size of the input.

There are many other notations but that’s a general idea of how Big O notation works. One thing to keep in mind as you think about Big O notation is that it is calculated based on the approximate worst-case performance. For example, if your program is searching through an array for a number, it assumes it won’t find what it’s looking for in the first element. It will actually assume that it won’t find what it’s looking for at all.

Here is a link to a <a href="http://bigocheatsheet.com/"> cheat sheet</a> that shows types of operations / algorithms and their respective big-o complexities. Enjoy!
